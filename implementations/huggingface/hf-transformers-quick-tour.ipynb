{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d934f6b4",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers Quick Tour\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/quicktour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4ec730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers datasets librosa torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395dbd3e",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d03752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
    "\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eabd6381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the latest cached version of the module from /Users/gen/.cache/huggingface/modules/datasets_modules/datasets/PolyAI--minds14/aa40414f15e0f919231d617440192034af844835dc1e6a697f4b552e0551fd26 (last modified on Fri Jan 13 13:20:56 2023) since it couldn't be found locally at PolyAI/minds14., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset minds14 (/Users/gen/.cache/huggingface/datasets/PolyAI___minds14/en-US/1.0.0/aa40414f15e0f919231d617440192034af844835dc1e6a697f4b552e0551fd26)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import pipeline\n",
    "\n",
    "# info on hf pipelines: https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline\n",
    "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Dataset: https://huggingface.co/datasets/PolyAI/minds14\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125e4d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FODING HOW I'D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I THURN A JOIN A COUNT']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': '5 stars', 'score': 0.7272654175758362}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the sampling rate\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n",
    "result = speech_recognizer(dataset[:4][\"audio\"])\n",
    "print([d[\"text\"] for d in result])\n",
    "# if data big then use generator instead of a list\n",
    "# eg: speech or vision\n",
    "# loads inputs to mem\n",
    "\n",
    "use_pytorch = True\n",
    "use_tensorflow = False\n",
    "\n",
    "# choose a suitable model from the hub: https://huggingface.co/models\n",
    "# For a custom use case, fine tune a model on your custom data\n",
    "# here: mBERT for French sentiment analysis\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "\n",
    "\n",
    "# Docs auto seq class: https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification\n",
    "# Docs auto tokenizer: https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "if use_pytorch:\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "elif use_tensorflow:\n",
    "    from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8db53",
   "metadata": {},
   "source": [
    "# The Auto Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82f0b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the inputs to ensure equal length\n",
    "if use_pytorch:\n",
    "    pt_batch = tokenizer(\n",
    "        [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "elif use_tensorflow:\n",
    "    tf_batch = tokenizer(\n",
    "        [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"tf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2a60561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [101, 16902, 24419, 10107, 10860, 41838, 12204, 10102, 22812, 40452, 10106, 26759, 100, 58263, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n",
      "        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# A note about auto classes and auto models\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Instantiate a tokenizer on the same model you'll use.\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(encoding)\n",
    "\n",
    "# This model is multilingual. Can use it to tokenize French too.\n",
    "french_encoding = tokenizer(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ðŸ¤— Transformers.\")\n",
    "print(french_encoding)\n",
    "\n",
    "\n",
    "# Auto model\n",
    "# docs: https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel\n",
    "\n",
    "if use_pytorch:\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    pt_outputs = pt_model(**pt_batch)\n",
    "\n",
    "    from torch import nn\n",
    "    \n",
    "    # normalization delayed so it can be combined with evaluation/loss function\n",
    "    # model outputs can behave like a tuple or dictionary (if keyed)\n",
    "    pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "    print(pt_predictions)\n",
    "elif use_tensorflow:\n",
    "    from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "    model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    tf_outputs = tf_model(tf_batch)\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "    tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
    "    tf_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b07ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tf_save_directory = \"./tf_save_pretrained\"\n",
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "\n",
    "if use_pytorch:\n",
    "    # Save a fine tuned model and tokenizer\n",
    "    tokenizer.save_pretrained(pt_save_directory)\n",
    "    pt_model.save_pretrained(pt_save_directory)\n",
    "    \n",
    "    # Load it back in later\n",
    "    pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n",
    "    \n",
    "\"\"\"\n",
    "elif use_tensorflow:\n",
    "    # Save model and tokenizer    \n",
    "    tf_save_directory = \"./tf_save_pretrained\"\n",
    "    tokenizer.save_pretrained(tf_save_directory)\n",
    "    tf_model.save_pretrained(tf_save_directory)\n",
    "    \n",
    "    # Load in model and tokenizer\n",
    "    tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n",
    "\"\"\"\n",
    "\n",
    "# Convert between frameworks from_pt or from_tf\n",
    "\n",
    "use_pytorch = False\n",
    "use_tensorflow = True\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "if use_pytorch:\n",
    "    from transformers import AutoModel\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
    "    pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n",
    "elif use_tensorflow:\n",
    "    from transformers import TFAutoModel\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "    tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n",
    "    \n",
    "use_pytorch = True\n",
    "use_tensorflow = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b12a5",
   "metadata": {},
   "source": [
    "# Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff78d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any model created with a custom config, weights need to be learned\n",
    "\n",
    "# Using a pretrained model, pull its config, \n",
    "# changing one attribute: number of attention heads\n",
    "from transformers import AutoConfig\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\", n_heads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b544454",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_pytorch:\n",
    "    from transformers import AutoModel\n",
    "    my_model = AutoModel.from_config(my_config)\n",
    "elif use_tensorflow:\n",
    "    from transformers import TFAutoModel\n",
    "    my_model = TFAutoModel.from_config(my_config)\n",
    "    \n",
    "# and then train it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3ea8b",
   "metadata": {},
   "source": [
    "# Trainer (pytorch optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3e2fb77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df257ef719ad49a9ad34b3edbd042379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fe039f066d4556a91369e8c4758212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1737ae0b88b4b369e1998a1ce102e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0a51db1f1c4bda9090872ffca055d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset rotten_tomatoes/default to /Users/gen/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38254f58d79d4de8819254f7a1dbb78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/488k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c504f1b12ee04730bc1927a5b210bd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7041b630361469c9d226e5a36de9337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset rotten_tomatoes downloaded and prepared to /Users/gen/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f8af0321164f5ca0582d3605b0eba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad73f7ae0f2405588043b6b06ebf7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0400f85d2c9d4cd4bf72db8149bdb36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a59946f08945048b547150fb19f672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainer Params\n",
    "\n",
    "# Model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "# Args\n",
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"trainer/out\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")\n",
    "\n",
    "\n",
    "# Preprocessing class like a tokenizer, image processor, feature extractor, or processor\n",
    "from transformers import AutoTokenizer\n",
    "# tokenizer from that model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT\n",
    "\n",
    "\n",
    "# Tokenizer for dataset\n",
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)\n",
    "\n",
    "# Data Collator (with padding)\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53208abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")  # doctest: +SKIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645228e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always have training in its own cell.\n",
    "# Don't want to have to retrain just cause the next thing didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67caadc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 8530\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2134\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2134' max='2134' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2134/2134 09:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.162000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trainer/out/checkpoint-500\n",
      "Configuration saved in trainer/out/checkpoint-500/config.json\n",
      "Model weights saved in trainer/out/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in trainer/out/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in trainer/out/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to trainer/out/checkpoint-1000\n",
      "Configuration saved in trainer/out/checkpoint-1000/config.json\n",
      "Model weights saved in trainer/out/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in trainer/out/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in trainer/out/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to trainer/out/checkpoint-1500\n",
      "Configuration saved in trainer/out/checkpoint-1500/config.json\n",
      "Model weights saved in trainer/out/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in trainer/out/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in trainer/out/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to trainer/out/checkpoint-2000\n",
      "Configuration saved in trainer/out/checkpoint-2000/config.json\n",
      "Model weights saved in trainer/out/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in trainer/out/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in trainer/out/checkpoint-2000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2134, training_loss=0.16697968784178543, metrics={'train_runtime': 560.2621, 'train_samples_per_second': 30.45, 'train_steps_per_second': 3.809, 'total_flos': 195799234032192.0, 'train_loss': 0.16697968784178543, 'epoch': 2.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5edfc634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOSError: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\\ngit init && git remote add origin && git pull origin main\\n or clone repo to a new folder and move your existing files there afterwards.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.push_to_hub() \n",
    "# This requires that it be a git repo\n",
    "\n",
    "\"\"\"\n",
    "OSError: Tried to clone a repository in a non-empty folder that isn't a git repository. If you really want to do this, do it manually:\n",
    "git init && git remote add origin && git pull origin main\n",
    " or clone repo to a new folder and move your existing files there afterwards.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c29a68",
   "metadata": {},
   "source": [
    "# TensorFlow Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Preprocessing class like a tokenizer, image processor, feature extractor, or processor\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Function to optimize dataset\n",
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])  # doctest: +SKIP\n",
    "\n",
    "# Dataset\n",
    "dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP\n",
    "tf_dataset = model.prepare_tf_dataset(\n",
    "    dataset, batch_size=16, shuffle=True, tokenizer=tokenizer\n",
    ")  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb164e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "model.fit(dataset)  # doctest: +SKIP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
