{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbe67cdc",
   "metadata": {},
   "source": [
    "# Fine Tuning RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359868c9",
   "metadata": {},
   "source": [
    "link: https://towardsdatascience.com/news-category-classification-fine-tuning-roberta-on-tpus-with-tensorflow-f057c37b093\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c85355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import seaborn as sn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "885b1e65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9198417",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When running in Colab:\n",
    "\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "batch_size=32 * tpu_strategy.num_replicas_in_sync\n",
    "print('Batch size:', batch_size)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39a07cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data from json file\n",
    "data = [json.loads(line) for line in open('../../data/News_Category_Dataset_v3.json', 'r')] \n",
    "random.shuffle(data) #shuffle the data \n",
    "labels=[]\n",
    "headlines=[]\n",
    "texts=[]\n",
    "for line in data:\n",
    "    labels.append(line['category'])\n",
    "    headlines.append(line['headline'])\n",
    "    #Combine headline and description into a single text input\n",
    "    text=line['headline']+' '+line['short_description']\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e679383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_categories(labels):\n",
    "    aggregated=[]\n",
    "    for line in labels:\n",
    "        if line=='WORLDPOST' or line=='THE WORLDPOST':\n",
    "            aggregated.append('WORLD NEWS')\n",
    "        elif line=='PARENTING' or line=='PARENTS':\n",
    "            aggregated.append('PARENTS & PARENTING')\n",
    "        elif line=='ARTS' or line=='CULTURE & ARTS':\n",
    "            aggregated.append('ARTS & CULTURE')\n",
    "        elif line=='STYLE':\n",
    "            aggregated.append('STYLE & BEAUTY')\n",
    "        elif line=='GREEN' or line=='ENVIRONMENT':\n",
    "            aggregated.append('ENVIRONMENT & GREEN')\n",
    "        elif line=='HEALTHY LIVING' or line=='WELLNESS':\n",
    "            aggregated.append('WELLNESS & HEALTHY LIVING')\n",
    "        elif line=='COMEDY' or line=='ENTERTAINMENT':\n",
    "            aggregated.append('ENTERTAINMENT & COMEDY')\n",
    "        elif line=='TASTE' or line=='FOOD & DRINK':\n",
    "            aggregated.append('FOOD, DRINK & TASTE')\n",
    "        elif line=='COLLEGE' or line=='EDUCATION':\n",
    "             aggregated.append('COLLEGE & EDUCATION')\n",
    "        elif line=='SCIENCE' or line=='TECH':\n",
    "             aggregated.append('SCIENCE & TECH')\n",
    "        elif line=='BUSINESS' or line=='MONEY':\n",
    "            aggregated.append('BUSINESS & MONEY')\n",
    "\n",
    "        else:\n",
    "            aggregated.append(line)\n",
    "    return aggregated\n",
    "\n",
    "aggregated=aggregate_categories(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "976f8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=sorted(list(set(labels))) #set will return the unique different entries\n",
    "n_categories=len(categories)\n",
    "\n",
    "def indicize_labels(labels):\n",
    "    \"\"\"Transforms string labels into indices\"\"\"\n",
    "    indices=[]\n",
    "    for j in range(len(labels)):\n",
    "        for i in range(n_categories):\n",
    "            if labels[j]==categories[i]:\n",
    "                indices.append(i)\n",
    "    return indices\n",
    "\n",
    "indices=indicize_labels(aggregated) #Integer label indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a5573",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# For Tensor Flow\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\") #Tokenizer\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='tf') #Tokenized text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "572c5a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\") #Tokenizer\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt') #Tokenized text for pytorch\n",
    "# docs: https://huggingface.co/docs/transformers/preprocessing\n",
    "\n",
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72a3ac09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 16215, 19716,  ...,     1,     1,     1],\n",
       "         [    0, 34985,   876,  ...,     1,     1,     1],\n",
       "         [    0,   108,   574,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    0,  6715,  5066,  ...,     1,     1,     1],\n",
       "         [    0, 11773,    17,  ...,     1,     1,     1],\n",
       "         [    0,  9690, 28514,  ...,     1,     1,     1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e3fb7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:248\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'size'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#dataset=tf.data.Dataset.from_tensor_slices((dict(inputs), indices)) #Create a pytorch dataset\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Create a pytorch dataset\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:250\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#dataset=tf.data.Dataset.from_tensor_slices((dict(inputs), indices)) #Create a pytorch dataset\n",
    "\n",
    "dataset=torch.utils.data.TensorDataset(inputs, indices) #Create a pytorch dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d80fa025",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#train test split, we use 10% of the data for validation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m val_data_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m\u001b[38;5;241m*\u001b[39mn_elements)\n\u001b[0;32m----> 6\u001b[0m val_ds\u001b[38;5;241m=\u001b[39m\u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mtake(val_data_size)\u001b[38;5;241m.\u001b[39mbatch(batch_size, drop_remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \n\u001b[1;32m      7\u001b[0m train_ds\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mskip(val_data_size)\u001b[38;5;241m.\u001b[39mbatch(batch_size, drop_remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m train_ds\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mAUTOTUNE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "n_elements=len(headlines)\n",
    "\n",
    "\n",
    "#train test split, we use 10% of the data for validation\n",
    "val_data_size=int(0.1*n_elements)\n",
    "val_ds=dataset.take(val_data_size).batch(batch_size, drop_remainder=True) \n",
    "train_ds=dataset.skip(val_data_size).batch(batch_size, drop_remainder=True)\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7997bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b60c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7335e93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240a7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50519f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e025427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41facd66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb51296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
